\documentclass{article}
\usepackage{graphicx} 
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[style=numeric,bibstyle=numeric,backend=biber,natbib=true,maxbibnames=99,giveninits=true,uniquename=init]{biblatex}

\addbibresource{references.bib}

\title{In search for the decent person: imposing intersectional group fairness to a credit decision model}

\begin{document}

\maketitle

\section*{Introduction}

A focal point in the machine learning community recently has 
been to make algorithmic treatment fair. This has led to legistlation related 
to the area being or prepared, most notably being the EU fair AI act \cite{Aiact}.

One of the central problems in enforcing fairness is that
 lots of datasets encode previous biases. Left 
unchecked, these are transferred to any model built on them. For this reason,
there has been an active development of fairness metrics \cite{stanley}.
These generally choose one trait used for discrimination and criterion for fairness, demonstrate 
the existence of discrimination, and correct for it based on this one trait.
In this work, we consider a setting closer to a real application case, where 
we need to consider many traits at once. To measure fairness in this setting, 
we define the measure of intersectional group fairness, and correct a 
credit-predicting random forest model to meet this fairness criterion.

The rest of this report is organized as follows. We start by outlining the 
context of the problem, and by describing the machine learning solution used.
Then, we define the fairness criterion of intersectional group fairness and 
use the criterion for evaluating our initial model. Then, we use the ... 
mechanism to correct the fairness gap.

\section*{Context}

The imagined setting studied in this project is as follows. 
A credit institution is building their first model for prediction whom to give credit to.
In light of the new legistlation they want to 
make sure that the regulations are not compromised when introducing 
algorithmic decision-making. Additionally, the company managers are interested 
in if the new rules can be exploite to their financial advantage: if 
imposing fairness could on one hand make sure that the model is lawful, but on 
the other hand also increase company profits. A team of analysts are 
tasked to investigate these matters.

\section*{Technical setting: a random forest for credit default prediction}

The dataset used is the German credit dataset \cite{stanley}.
Notably, the dataset is from 1994, which makes its predictive power for the future 
less certain, especially with respect to fairness: equality has greatly advanced 
between now and the date of the dataset.
Of the features available in the data, some are nowadays considered 
discriminatory as features in decision-making: marital status, 
sex, age, and immigrant status.
Of these, marital status was omitted from the analysis because of all females being noted as 
married or widowed. Lastly, a significant constraint from the data 
is its size: there are only 1000 rows. 

The data was preprocessed by encoding categorical features into one-hot encoded 
dummy or numerical variables depending on the ordinality of each variable.
Age, used as a sensitive variable in this analysis, was turned to a binary feature by splitting it to less or more than 25 years. 
A summary of encodings can be found in table 1 \cite{stanley}. For hyperparameter 
optimization, a simple grid search was performed, and ... were found to result in the highest accuracy.

The model type used to predict credit defaults was a random forest, since the 
dataset description named it as having the best baseline accuracy \cite{stanley}.
The accuracy listed as baseline, 78\%, was achieved with our initial model.
For testing fairness levels, leave-one-out cross-validation was performed to obtain 
a test prediction for each data point. This was opted for instead of a separate test set 
because of the small dataset size: we assumed that slight deviation of the model because of leaving out one data point 
is a better approximation than measuring fairness on a small test set and not using all data for training.

Specifically, we used the sklearn v. version here RandomForest \cite{stanley} for classification, 
and cross-val-predict for obtaining test predictions for all data points in 
the dataset.

huom table 2 laita sensitive or not (used for fairness analysis)

\section*{The fairness condition: intersectional group fairness}

To motivate the choice of fairness constraint, we trace back to our goal and assumptions.
 The goal of a credit model is to output a probability that 
expresses one's likelihood of paying back their credit. Thus, the company objective is to find 
the decent people: those who duly take care of their resposibilities.

The assumption of modeling on the German credit dataset is that the historical data of the mapping from a person's financial status and personal details
to creditworthiness is truthful: that those granted credit in the dataset are the least likely to default.

However, there are well-justified reasons to believe that this is not the case.
Since no ground truth on default rates is given, it is reasonable to bring about the hypothesis that
the data penalizes disadvantaged groups in credit decisions because of blunt discrimination.
One example of why this is reasonable is the known pattern that eqully qualified females tend to be paid worse 
than men. On the other hand, it seems reasonable to assume that the relationship between nonsensitive
features (features related to financial status) and the target has real predictive power in the present.
An additional assumption we make is that disadvantages compound: that those belonging to multiple traditionally 
discriminated groups, such as immigrant females, experience the strongest acts of discrimination, and that the
reliability of their credit decision in historical data does not equal that of another group with only a single 
disadvantageous trait.

To measure the level of fairness, we divide the population 
into groups based on the assumed disadvantages each subject has: whether one is female, under 25 years old or of immigrant background. Then, we apply standard group fairness
to each group to check if equal proportions of groups with different disadvantages are granted credit. 
A key insight to note here is that we are also aiming for better profits:
 we hypothesize that our way of finding people to grant 
credit to leads to lower default rates than relying on the truthfulness of 
past decisions: we try to improve the profit margin by 
not declining credit for good applicants because of untrue stereotypes.

Formally, intersectional group fairness is defined as followings:

definition: intersectional group fairness 
1. divide population by disadvantage / other. proportion of credit should be equal
disadvantage: all nonempty subsets of the set of disadvantages

Implementing this fairness condition has several advantages. Trivially, we achieve group fairness. For individual fairness,
if the assumption of the traits considered to cause discrimination holds (eg. that immigrant population is not 
in reality better off, an unlikely state of affairs), we are guaranteed to only compromise individual fairness in 
ways that can be justified as affirmative action, making this a form of lawful positive discrimination.
This results from the fact that we correct for group fairness by artificially granting more credit to 
groups discriminated against in the group fairness sense, thus we only change decisions for members disadvantaged groups for the better.
Lastly, an additional benefit for the credit institution is that doing affirmative action by enforcing fairness
can be used as a marketing and branding argument, which could improve the public reputation of the institution.

\subsection*{Pre-remedy fairness level}

The level of group fairness was measured by splitting the data set in two, 
one being the disadvantaged group and one for all other subjects. Then, 
credit grant rates were compared between the groups. This was conducted for all 
combinations of the disadvantages. The results for these experiments are listed in table 3. \cite{stanley}

The most significant insights we can see from the table is that the hypothesis of compounding seems to hold:
those with more disadvantages are worse off. 
Additionally, for the individual traits,  age seems 
to cause the most unfair treatment, whereas sex has the least impact on creditworthiness.

\subsection*{Imposing fairness: correction mechanism here}
remedying fairness 
technique - rationale for using - results
for a few techniques, find the best one

rationale for pre/postprocessing
simpler to implement for a complex model
    output modification is quite justified legally, is explainable and can be backed as affirmative action.

preprocessing technique 

postprocessing technique
combinations of sensitive variables
    divide by these into 8 nonoverlapping groups 
order by class probability 
take ratio of first from the queue and grant -> ratio is easy to change

Results of postprocessing.
ratios vary quite a bit 
data imbalance
accuracy is not reduced too much (trade wrong predictions for other wrongs, not great model so we cannot really know 
how much accuracy we trade off)
we flip about 9 percent of labels. is not thaaat much 

results in table.
    percentage of labels flipped in preprocessing
    percentage of labels flipped in postprocessing
    ratio

table 2 accuracy scores
0.77 0.77 0.64

\subsection*{Conclusion}

conclusion: suggestions for the institution

see how business metrics evolve to see if our hypothesis of better or equal profits holds

which corrections made sense? did they remedy the problem?
did the problem make sense?

Tradeoff of fairness to accuracy loss is good. you should definitely do these corrections
satisfying a large fraction of fairness, group fairness and compromise individual fairness only as affirmative 
action -> marketing argument
the problem was good because you cannot consider stuff in isolation in real life

to improve we need more representative, balanced data -> ehen doing further decisions, save the data and decision made to 
iteratively finetune the process. model fairness could be experimented but the downside is explainability (rf not super 
explainable)

general conclusion
fairness is important?
you dont need magic tricks to implement fairness, simple tricks are good since they are explainable,
    esp in case of a complex model

In this report, we investigated how to enforce group fairness for all sensitive groups present in a data set 
simultaneously. The central goal of this was to achieve a fair model that is lawful, explainable and 
profit-maximizing for the institution. The result indicates that this is possible; based on our dataset, it seems that 
fairness can be achieved with a reasonably accurate model. Thus, our central message to the credit institution
is that the fairness-corrected model is definitely beneficial: its premises of nondiscrimination in a lawful and 
explainable way far outweigh the accuracy loss.

As notes on further improvements, the most important next step would be to acquire more data. Also, this data should 
be more representative of all subgroups presented in this report to find less individual data point -depending credit grant thresholds for each group.
Additionally, model optimization with fairness constraints could be experimented with; an interesting hypothesis to experiment with is 
to only add a constraint for group fairness only for the group with all disadvantages and see how far this 
constraint enforces global intersectional group fairness.

As a general conclusion, a key insight to note is that to enforce fairness, one does not necessarily need highly complex and advanced 
procedures. On the contrary, for more complex but accurate models, such as random forests used here, the simpler techniques can
lead to greater explainability and ultimately a more trustworthy result.

\subsection*{Abstract}

credit model for a credit institution to achieve group fairness
we find unfairness
we use simple pre/postprocessing to achieve prefect group fairness in an explainable and lawful way, accuracy reduced only 0.77 to 0.764.
real world implementable case

While algorithmic decision-making is gaining popularity, it is important to make sure
that no unfair treatment is introduced to any group of people. In this project we investigate 
group fairness of a credit decision model with multiple definitions of the groupings 
simultaneously, a metric we formalize as intersectional group fairness. We find that 
the initial model encodes discrimination, and use simple pre- and postprocessing adjustments 
to correct these faults fully while only reducing accuracy from 0.77 to 0.64. The central insight from this is 
that interaction of sensitive variables has to be considered to achieve a fair model applicable to a real-world use case.

\section*{notes}

technicalities 

dataset we used, most significant characteristics
    - genders and marital statuses
    - 1000 rows
    - old -> imposing equality for relevance

the model we used
    random forest
        reason: dataset stated that it has the best baseline accuracy of about 78% 
            which we also achieved
    hyperparameter optimization: simple grid search
        params obtained and short description of their significance
    class encoding: one hot / self reasoned numericals, discussion on 
        eg if being in unpaid housing is better creditwise than renting  
        
sklearn random forest, sklearn loo cv


goal for profit: find the most decent people

assumption of modeling: you can use data to get the target.
    problem with old data: encodes nontrue stereotypes, such as women default, young default 
    so: assumption may be broken?

hypothesis: order people by decentness within stereotypical groups.
cutoff, above that, grant credit -> you get the most decent people 
    key insight: this should also maximize profit
grouping by how much stereotypical discrimination
    intersectionality: more disadvantages compound to more discrimination    

additional +'s: profit, affirmative action, law

also, affirmative action. additional +, brand prestige
we dont know if this is true but we can check business metrics after implementing 
    the model.
question: stereotyped groups are more decent than is assumed by data and nonfair models

also, law
law: requires individual fairness. one with similar nonsensitives should 
    have equal decision 
group fairness causes discrimination in individual fairness sense to advantaged groups 
    allowed in law for affirmative action 

elisiishetkonenmitä
mitä yritän perustella: miksi intersectional group fairness on järkevä ratkaisu
perusteet
    data on biased eli datan päätökset ei oo ne jotka on oikeesti kaikista parhaita
        data: who is assumed to pay back duly, true: who pays back duly 
        we have good reasons to suspect these are not the same people. that discriminated people are actually better
        syy (oletus) syrjintä
        peruste: tiedetyt palkkaerot (eti reference)
    suhde financial status -> creditworthy on järkevä
    jos järjestät statuksen mukaan et saa parhaita
        jos järjestätä statuksen mukaan syrjintäkorjauksella saat parhaat

        intro 
        1. establish a territory: field studied
credit needs to be fair 
eu law for equal ai
2. establishing a niche: what is the gap filled
data has biases
definition of fairness, some condition eg gender 
then manipulate data/model/output to make it so that condition is met 
3. occupying the niche: filling the gap
but what about the young disabled female immigrant?
considering only one trait at a time does not suffice
in this work we consider seeing for everything in credit context
we are able to 

\printbibliography

\end{document}